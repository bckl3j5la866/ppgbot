import json
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
import re
import logging
from config import DOCUMENTS_DB_FILE as DB_FILE

logger = logging.getLogger(__name__)

# –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞
STOP_WORDS = {
    '–ø—Ä–∏–∫–∞–∑', '—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ', '–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ', '–∑–∞–∫–æ–Ω', '–ø—Ä–æ–µ–∫—Ç',
    '–æ—Ç', '‚Ññ', '–≥', '–≥–æ–¥–∞', '–ª–µ—Ç', '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π', '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–æ–π',
    '—Å–ª—É–∂–±—ã', '–ø–æ', '–Ω–∞–¥–∑–æ—Ä—É', '–≤', '—Å—Ñ–µ—Ä–µ', '–æ–±–ª–∞—Å—Ç–∏', '—Å',
    '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è', '–∏', '–Ω–∞—É–∫–∏', '—Ä–æ—Å—Å–∏–π—Å–∫–æ–π', '—Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏',
    '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–∞', '–ø—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è', '—Ä–µ—Å–ø—É–±–ª–∏–∫–∏', '—Å–∞—Ö–∞', '—è–∫—É—Ç–∏—è',
    '–æ–±', '—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏', '–≤–Ω–µ—Å–µ–Ω–∏–∏', '–∏–∑–º–µ–Ω–µ–Ω–∏–π', '–Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö',
    '–æ—Ç–¥–µ–ª—å–Ω—ã—Ö', '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω', '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ'
}

def init_database():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
    os.makedirs('data', exist_ok=True)
    if not os.path.exists(DB_FILE):
        initial_data = {
            "documents": [],
            "last_update": "",
            "created_at": datetime.now().isoformat()
        }
        with open(DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(initial_data, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {DB_FILE}")

def get_documents(organization: str = None) -> List[Dict[str, Any]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã, —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –±–µ–∑"""
    init_database()
    try:
        with open(DB_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            documents = data.get('documents', [])
    except (FileNotFoundError, json.JSONDecodeError):
        documents = []

    if organization:
        return [doc for doc in documents if doc.get('organization') == organization]
    return documents

def get_document_count() -> int:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ"""
    return len(get_documents())

def add_documents(new_documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑—É, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–æ–≤—ã–µ"""
    init_database()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    try:
        with open(DB_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            existing_documents = data.get('documents', [])
    except (FileNotFoundError, json.JSONDecodeError):
        data = {
            "documents": [],
            "last_update": "",
            "created_at": datetime.now().isoformat()
        }
        existing_documents = []

    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    cleaned_new_docs = []
    for doc in new_documents:
        # –ï—Å–ª–∏ –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏–º–µ–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤—Å–µ–π –±–∞–∑—ã, –∏–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Å—Å–∏–≤ documents
        if isinstance(doc, dict) and 'documents' in doc:
            cleaned_new_docs.extend(doc.get('documents', []))
        # –ï—Å–ª–∏ —ç—Ç–æ –º–∞—Å—Å–∏–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
        elif isinstance(doc, list):
            cleaned_new_docs.extend(doc)
        # –ï—Å–ª–∏ —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
        elif isinstance(doc, dict) and 'url' in doc:
            cleaned_new_docs.append(doc)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ URL
    existing_urls = {doc['url'] for doc in existing_documents}
    truly_new_docs = [doc for doc in cleaned_new_docs if doc['url'] not in existing_urls]

    if truly_new_docs:
        logger.info(f"üì• –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(truly_new_docs)} –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É - –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ
        data['documents'] = existing_documents + truly_new_docs
        data['last_update'] = datetime.now().isoformat()
        
        if not data.get('created_at'):
            data['created_at'] = datetime.now().isoformat()
        
        with open(DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ –ë–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞. –¢–µ–ø–µ—Ä—å {len(data['documents'])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    else:
        logger.info("‚úÖ –ù–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    return truly_new_docs

def get_latest_documents(organization: str = None, limit: int = 5) -> List[Dict[str, Any]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)"""
    documents = get_documents(organization)
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (—É–±—ã–≤–∞–Ω–∏–µ)
    documents.sort(key=lambda x: x.get('publishDate', ''), reverse=True)
    return documents[:limit]

def search_documents(query: str, limit: int = 10) -> List[Dict[str, Any]]:
    """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Å—Ç–æ–ø-—Å–ª–æ–≤"""
    documents = get_documents()
    query_lower = query.lower()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–ª–æ–≤–∞ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    query_words = [word for word in query_lower.split() 
                  if word not in STOP_WORDS and len(word) > 2]
    
    # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if not query_words:
        return []
    
    results = []
    for doc in documents:
        title = doc.get('documentTitle', '').lower()
        organization = doc.get('organization', '').lower()
        
        # –ò—â–µ–º –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–ª–æ–≤–∞ (–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –ò)
        if all(word in title or word in organization for word in query_words):
            results.append(doc)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
    results.sort(key=lambda x: x.get('publishDate', ''), reverse=True)
    
    return results[:limit]

def get_last_update_date() -> Optional[datetime]:
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–∞–∑—ã"""
    try:
        with open(DB_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            last_update_str = data.get('last_update')
            if last_update_str:
                return datetime.fromisoformat(last_update_str)
    except (FileNotFoundError, KeyError, ValueError):
        pass
    return None

def set_last_update_date(date: datetime) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–∞–∑—ã"""
    init_database()
    try:
        with open(DB_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        data = {
            "documents": [],
            "last_update": "",
            "created_at": datetime.now().isoformat()
        }
    
    data['last_update'] = date.isoformat()
    
    with open(DB_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def check_database_integrity() -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    init_database()
    
    try:
        with open(DB_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        return {"error": f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã: {e}"}
    
    documents = data.get('documents', [])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
    urls = [doc.get('url') for doc in documents if doc.get('url')]
    unique_urls = set(urls)
    duplicates = len(urls) - len(unique_urls)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
    valid_docs = 0
    missing_fields = []
    
    for doc in documents:
        if all(key in doc for key in ['organization', 'documentTitle', 'url', 'publishDate']):
            valid_docs += 1
        else:
            missing_fields.append(doc.get('url', 'unknown'))
    
    return {
        "total_documents": len(documents),
        "unique_urls": len(unique_urls),
        "duplicate_documents": duplicates,
        "valid_documents": valid_docs,
        "documents_with_missing_fields": len(documents) - valid_docs,
        "last_update": data.get('last_update'),
        "created_at": data.get('created_at')
    }