import logging
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
from pathlib import Path
from typing import List, Dict, Any
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import io
import re
from datetime import datetime
from database import add_documents, get_documents, get_last_update_date, set_last_update_date

logger = logging.getLogger(__name__)

def parse_date(date_str: str) -> datetime:
    """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY"""
    try:
        return datetime.strptime(date_str, "%d.%m.%Y")
    except (ValueError, TypeError):
        return datetime.min

class OpenDataParser:
    # URL –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ)
    CSV_URLS = {
        30: "http://publication.pravo.gov.ru/opendata/7710349494-legalacts-30/data-legalacts-30.csv",
        90: "http://publication.pravo.gov.ru/opendata/7710349494-legalacts-90/data-legalacts-90.csv", 
        360: "http://publication.pravo.gov.ru/opendata/7710349494-legalacts-360/data-legalacts-360.csv"
    }
    
    BASE_URL = "http://publication.pravo.gov.ru"
    
    TARGET_ORGS = {
        "a86f12ae-1908-4059-86a5-0803ea08f5ec": "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –ø—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
        "39ec279e-970f-43c0-85b7-4aba57163bb7": "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –°–∞—Ö–∞ (–Ø–∫—É—Ç–∏—è)",
        "6fcb828d-55bf-4e1f-bb57-f9ce6cfefc0d": "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –ø–æ –Ω–∞–¥–∑–æ—Ä—É –≤ —Å—Ñ–µ—Ä–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏"
    }

    def __init__(self, limit: int = 500, days: int = 30):
        self.limit = limit
        self.days = days
        self.csv_url = self.CSV_URLS.get(days, self.CSV_URLS[30])
        Path("data").mkdir(exist_ok=True)

    def fetch_csv(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç CSV —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥"""
        logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞ {self.days} –¥–Ω–µ–π...")
        session = requests.Session()
        retry = Retry(total=3, backoff_factor=5, status_forcelist=[500, 502, 503, 504])
        session.mount("http://", HTTPAdapter(max_retries=retry))
        session.mount("https://", HTTPAdapter(max_retries=retry))
        
        try:
            response = session.get(self.csv_url, timeout=60)
            response.raise_for_status()
            df = pd.read_csv(io.StringIO(response.text), sep=';', encoding='utf-8')
            logger.info(f"‚úÖ CSV –∑–∞ {self.days} –¥–Ω–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω ({len(df)} —Å—Ç—Ä–æ–∫)")
            return df
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV –∑–∞ {self.days} –¥–Ω–µ–π: {e}")
            return pd.DataFrame()

    def get_department_links(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ü–µ–ª–µ–≤—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        df = self.fetch_csv()
        if df.empty:
            return {}

        df.columns = [col.strip() for col in df.columns]
        
        links = {}
        for org_id, org_name in self.TARGET_ORGS.items():
            org_row = df[df['ID –ø—Ä–∏–Ω—è–≤—à–µ–≥–æ –æ—Ä–≥–∞–Ω–∞'] == org_id]
            
            if not org_row.empty:
                link = org_row.iloc[0]['–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤']
                if pd.notna(link):
                    clean_link = str(link).strip()
                    full_url = urljoin(self.BASE_URL, clean_link)
                    links[org_name] = full_url
                    logger.info(f"‚úÖ –°—Å—ã–ª–∫–∞ –¥–ª—è {org_name}")
        
        return links

    def clean_url_from_anchor(self, url: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç —è–∫–æ—Ä—å –∏–∑ URL –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""
        # –£–¥–∞–ª—è–µ–º —è–∫–æ—Ä—å
        clean_url = url.split('#')[0]
        
        # –ü–∞—Ä—Å–∏–º URL –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        parsed = urlparse(clean_url)
        query_params = parse_qs(parsed.query)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —è–∫–æ—Ä—å —Å index, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä
        if '#' in url:
            anchor_part = url.split('#')[-1]
            if 'index=' in anchor_part:
                index_match = re.search(r'index=(\d+)', anchor_part)
                if index_match:
                    query_params['index'] = [index_match.group(1)]
        
        # –°–æ–±–∏—Ä–∞–µ–º URL –æ–±—Ä–∞—Ç–Ω–æ
        new_query = urlencode(query_params, doseq=True)
        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{new_query}"
        
        return clean_url

    def get_next_page_url(self, current_url: str, soup: BeautifulSoup) -> str:
        """–ò—â–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
        # –ò—â–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é –ø–æ –∫–ª–∞—Å—Å—É page-navigation
        pagination = soup.find(class_='page-navigation')
        
        if not pagination:
            logger.warning("‚ùå –ü–∞–≥–∏–Ω–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
            return None

        # –ò—â–µ–º —Å—Å—ã–ª–∫—É —Å title "–°–ª–µ–¥—É—é—â–∞—è"
        next_link = pagination.find('a', attrs={'title': '–°–ª–µ–¥—É—é—â–∞—è'})
        
        if not next_link:
            # –ò—â–µ–º –ø–æ –∏–∫–æ–Ω–∫–µ —Å—Ç—Ä–µ–ª–∫–∏ –≤–ø—Ä–∞–≤–æ
            next_icon = pagination.find('i', class_='fas fa-caret-right')
            if next_icon:
                next_link = next_icon.find_parent('a')

        if next_link and next_link.get('href'):
            href = next_link.get('href')
            logger.info(f"üîó –ù–∞–π–¥–µ–Ω–∞ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞, href: {href}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º href –≤–º–µ—Å—Ç–æ onclick
            full_url = urljoin(self.BASE_URL, href)
            
            # –û—á–∏—â–∞–µ–º URL –æ—Ç —è–∫–æ—Ä—è –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            clean_url = self.clean_url_from_anchor(full_url)
            logger.info(f"‚úÖ –û—á–∏—â–µ–Ω–Ω—ã–π URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {clean_url}")
            
            return clean_url

        logger.warning("‚ùå –°—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return None

    def parse_department_documents_with_pagination(self, url: str, org_name: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –≤–µ–¥–æ–º—Å—Ç–≤–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –¥–ª—è {org_name}")
        
        session = requests.Session()
        retry = Retry(total=3, backoff_factor=2, status_forcelist=[500, 502, 503, 504])
        session.mount("http://", HTTPAdapter(max_retries=retry))
        session.mount("https://", HTTPAdapter(max_retries=retry))

        all_docs = []
        seen_urls = set()
        current_url = url
        page_count = 0
        max_pages = 20
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        while current_url and page_count < max_pages:
            page_count += 1
            logger.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_count}: {current_url}")
            
            try:
                response = session.get(current_url, timeout=30, headers=headers)
                response.raise_for_status()
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {current_url}: {e}")
                break

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_docs = self.parse_single_page(soup, org_name, seen_urls)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –Ω–∞—á–∞–ª–æ, —Ç–∞–∫ –∫–∞–∫ –ø–∞–≥–∏–Ω–∞—Ü–∏—è –∏–¥–µ—Ç –æ—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º
            all_docs = page_docs + all_docs
            
            logger.info(f"üìë –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_count}: {len(page_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
            if self.limit and len(all_docs) >= self.limit:
                all_docs = all_docs[:self.limit]
                logger.info(f"‚ö° –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {self.limit} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                break
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            next_url = self.get_next_page_url(current_url, soup)
            if not next_url:
                logger.info(f"üõë –ü–∞–≥–∏–Ω–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_count}")
                break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞—Ü–∏–∫–ª–∏–ª–∏—Å—å –ª–∏ –º—ã
            if next_url == current_url:
                logger.info(f"üõë –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ü–∏–∫–ª, –ø–∞–≥–∏–Ω–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                break
                
            current_url = next_url

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–æ—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º)
        all_docs.sort(key=lambda x: parse_date(x.get('publishDate', '01.01.1970')), reverse=True)
        
        logger.info(f"‚úÖ –í—Å–µ–≥–æ –¥–ª—è {org_name}: {len(all_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å {page_count} —Å—Ç—Ä–∞–Ω–∏—Ü")
        return all_docs

    def parse_single_page(self, soup: BeautifulSoup, org_name: str, seen_urls: set) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        docs = []

        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        all_links = soup.find_all('a', href=True)
        document_links = [link for link in all_links if '/document/' in link['href']]

        for link in document_links:
            href = link['href']
            full_url = urljoin(self.BASE_URL, href)
            
            if full_url in seen_urls:
                continue
            seen_urls.add(full_url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –¥–∞—Ç—É
            title = self.extract_document_title(link, soup)
            date = self.extract_document_date(link)

            doc = {
                "organization": org_name,
                "documentTitle": title,
                "publishDate": date,
                "url": full_url
            }
            
            docs.append(doc)

        return docs

    def extract_document_title(self, link, soup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        title = link.get_text(strip=True)
        
        if title and title != "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è":
            return title
        
        title_attr = link.get('title', '').strip()
        if title_attr:
            return title_attr
        
        # –ü–æ–∏—Å–∫ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
        parent = link.parent
        for _ in range(3):
            if parent:
                parent_text = parent.get_text(strip=True)
                if parent_text and len(parent_text) > 10:
                    return parent_text
                parent = parent.parent
        
        doc_id = link['href'].split('/')[-1] if '/' in link['href'] else link['href']
        return f"–î–æ–∫—É–º–µ–Ω—Ç {doc_id}"

    def extract_document_date(self, link) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        date = "–ë–µ–∑ –¥–∞—Ç—ã"
        
        # –ò—â–µ–º –¥–∞—Ç—É –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
        current_elem = link.parent
        for _ in range(5):
            if current_elem:
                text = current_elem.get_text()
                date_match = re.search(r'\d{2}\.\d{2}\.\d{4}', text)
                if date_match:
                    return date_match.group()
                current_elem = current_elem.parent
        
        return date

    def get_new_documents_with_boundary(self, source_key: str) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ–ø–µ—Ä—å –¥–µ–ª–∞–µ—Ç—Å—è –≤ notifier.py
        """
        dept_links = self.get_department_links()
        org_name = self.get_organization_name_by_key(source_key)
        
        if not org_name or org_name not in dept_links:
            return []
        
        url = dept_links[org_name]
        
        # –ü—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        all_docs = self.parse_department_documents_with_pagination(url, org_name)
        return all_docs

    def get_organization_name_by_key(self, source_key: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ –∫–ª—é—á—É"""
        mapping = {
            "federal": "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –ø—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "regional": "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –°–∞—Ö–∞ (–Ø–∫—É—Ç–∏—è)", 
            "rosobrnadzor": "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –ø–æ –Ω–∞–¥–∑–æ—Ä—É –≤ —Å—Ñ–µ—Ä–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏"
        }
        return mapping.get(source_key)

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    def get_new_documents_smart(self) -> List[Dict[str, Any]]:
        """–£–º–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        logger.info("üß† –£–º–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        
        last_update = get_last_update_date()
        if last_update:
            days_since_update = (datetime.now() - last_update).days
            if days_since_update <= 7:
                self.days = 30
                self.csv_url = self.CSV_URLS[30]
                logger.info("üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–∏–æ–¥ 30 –¥–Ω–µ–π (–±—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ)")
            else:
                self.days = 90
                self.csv_url = self.CSV_URLS[90]
                logger.info("üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–∏–æ–¥ 90 –¥–Ω–µ–π (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ)")
        else:
            self.days = 30
            self.csv_url = self.CSV_URLS[30]
            logger.info("üîç –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–∏–æ–¥ 30 –¥–Ω–µ–π")
        
        dept_links = self.get_department_links()
        if not dept_links:
            return []

        all_docs = []
        for org_name, url in dept_links.items():
            docs = self.parse_department_documents_with_pagination(url, org_name)
            all_docs.extend(docs)

        new_docs = add_documents(all_docs)
        set_last_update_date(datetime.now())
        
        logger.info(f"üìÇ –ù–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {len(new_docs)}")
        return new_docs

    def get_new_documents_quick(self) -> List[Dict[str, Any]]:
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ - —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π"""
        logger.info("‚ö° –ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (30 –¥–Ω–µ–π)")
        self.days = 30
        self.csv_url = self.CSV_URLS[30]
        
        dept_links = self.get_department_links()
        if not dept_links:
            return []

        all_docs = []
        for org_name, url in dept_links.items():
            docs = self.parse_department_documents_with_pagination(url, org_name)
            all_docs.extend(docs)

        new_docs = add_documents(all_docs)
        set_last_update_date(datetime.now())
        
        logger.info(f"üìÇ –ù–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {len(new_docs)}")
        return new_docs

    def get_new_documents_full(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ - 360 –¥–Ω–µ–π (–¥–ª—è –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è)"""
        logger.info("üîß –ü–æ–ª–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (360 –¥–Ω–µ–π)")
        self.days = 360
        self.csv_url = self.CSV_URLS[360]
        
        dept_links = self.get_department_links()
        if not dept_links:
            return []

        all_docs = []
        for org_name, url in dept_links.items():
            docs = self.parse_department_documents_with_pagination(url, org_name)
            all_docs.extend(docs)

        new_docs = add_documents(all_docs)
        set_last_update_date(datetime.now())
        
        logger.info(f"üìÇ –ù–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {len(new_docs)}")
        return new_docs

    def get_new_documents(self) -> List[Dict[str, Any]]:
        """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–º–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ"""
        return self.get_new_documents_smart()