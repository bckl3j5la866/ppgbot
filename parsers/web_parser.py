# parsers/web_parser.py
import logging
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
from pathlib import Path
from typing import List, Dict, Any
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re
from datetime import datetime

logger = logging.getLogger(__name__)

class WebParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–±–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""
    
    BASE_URL = "http://publication.pravo.gov.ru"
    
    # –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤–µ–¥–æ–º—Å—Ç–≤ (–∏–∑–±–µ–≥–∞–µ–º API)
    SOURCE_URLS = {
        "federal": "http://publication.pravo.gov.ru/Department/View/262?sort=PublicationDate_desc&page=1",
        "regional": "http://publication.pravo.gov.ru/Department/View/39ec279e-970f-43c0-85b7-4aba57163bb7?sort=PublicationDate_desc&page=1",
        "rosobrnadzor": "http://publication.pravo.gov.ru/Department/View/320?sort=PublicationDate_desc&page=1"
    }
    
    ORGANIZATION_NAMES = {
        "federal": "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –ø—Ä–æ—Å–≤–µ—â–µ–Ω–∏—è –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
        "regional": "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –°–∞—Ö–∞ (–Ø–∫—É—Ç–∏—è)",
        "rosobrnadzor": "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –ø–æ –Ω–∞–¥–∑–æ—Ä—É –≤ —Å—Ñ–µ—Ä–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏"
    }

    def __init__(self, limit: int = 50):
        self.limit = limit
        self.session = self._create_session()

    def _create_session(self):
        """–°–æ–∑–¥–∞–µ—Ç —Å–µ—Å—Å–∏—é —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        session = requests.Session()
        retry = Retry(
            total=3,
            backoff_factor=0.5,
            status_forcelist=[500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        return session

    def get_documents(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_documents = []
        
        for source_key, url in self.SOURCE_URLS.items():
            try:
                logger.info(f"üîÑ –ü–∞—Ä—Å–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫: {source_key}")
                docs = self.parse_department(url, source_key)
                all_documents.extend(docs)
                logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {source_key}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source_key}: {e}")
        
        return all_documents

    def parse_department(self, start_url: str, source_key: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤–µ–¥–æ–º—Å—Ç–≤–∞ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""
        org_name = self.ORGANIZATION_NAMES[source_key]
        all_docs = []
        current_url = start_url
        page_count = 0
        max_pages = 10  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        
        logger.info(f"üåê –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è {org_name}")
        
        while current_url and page_count < max_pages:
            page_count += 1
            logger.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_count}: {current_url}")
            
            try:
                response = self.session.get(current_url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                page_docs = self.parse_page(soup, org_name)
                all_docs.extend(page_docs)
                
                logger.info(f"üìë –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_count} –Ω–∞–π–¥–µ–Ω–æ {len(page_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
                if len(all_docs) >= self.limit:
                    all_docs = all_docs[:self.limit]
                    logger.info(f"‚ö° –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {self.limit} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                    break
                
                # –ò—â–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                next_url = self.find_next_page(soup, current_url)
                if not next_url:
                    logger.info(f"üõë –ü–∞–≥–∏–Ω–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_count}")
                    break
                    
                current_url = next_url
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {current_url}: {e}")
                break
        
        logger.info(f"‚úÖ –í—Å–µ–≥–æ –¥–ª—è {org_name}: {len(all_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return all_docs

    def parse_page(self, soup: BeautifulSoup, org_name: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        documents = []
        
        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        doc_containers = soup.find_all('div', class_=['document-item', 'doc-item'])
        
        if not doc_containers:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ - –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            doc_links = soup.find_all('a', href=lambda x: x and '/Document/View/' in x)
            for link in doc_links:
                doc = self.extract_document_from_link(link, org_name)
                if doc:
                    documents.append(doc)
            return documents
        
        # –ü–∞—Ä—Å–∏–º –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        for container in doc_containers:
            doc = self.extract_document_from_container(container, org_name)
            if doc:
                documents.append(doc)
        
        return documents

    def extract_document_from_container(self, container, org_name: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞"""
        try:
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
            link = container.find('a', href=lambda x: x and '/Document/View/' in x)
            if not link:
                return None
            
            href = link.get('href')
            full_url = urljoin(self.BASE_URL, href)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
            title = self.clean_text(link.get_text())
            if not title or title == "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è":
                title = link.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            
            # –ò—â–µ–º –¥–∞—Ç—É –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
            date = self.find_date_in_container(container)
            
            return {
                "organization": org_name,
                "documentTitle": title[:500],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                "url": full_url,
                "publishDate": date
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return None

    def extract_document_from_link(self, link, org_name: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ –æ–¥–∏–Ω–æ—á–Ω–æ–π —Å—Å—ã–ª–∫–∏"""
        try:
            href = link.get('href')
            full_url = urljoin(self.BASE_URL, href)
            
            title = self.clean_text(link.get_text())
            if not title or title == "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è":
                title = link.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –¥–∞—Ç—É –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
            date = self.find_nearby_date(link)
            
            return {
                "organization": org_name,
                "documentTitle": title[:500],
                "url": full_url,
                "publishDate": date
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ —Å—Å—ã–ª–∫–∏: {e}")
            return None

    def find_date_in_container(self, container) -> str:
        """–ò—â–µ—Ç –¥–∞—Ç—É –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –ò—â–µ–º –ø–æ –∫–ª–∞—Å—Å–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –¥–∞—Ç—É
        date_selectors = [
            '.document-date', '.doc-date', '.date', 
            '.publication-date', '.publish-date'
        ]
        
        for selector in date_selectors:
            date_elem = container.select_one(selector)
            if date_elem:
                date_text = self.clean_text(date_elem.get_text())
                date_match = re.search(r'\d{2}\.\d{2}\.\d{4}', date_text)
                if date_match:
                    return date_match.group()
        
        # –ò—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
        container_text = container.get_text()
        date_match = re.search(r'\d{2}\.\d{2}\.\d{4}', container_text)
        if date_match:
            return date_match.group()
        
        return "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

    def find_nearby_date(self, element) -> str:
        """–ò—â–µ—Ç –¥–∞—Ç—É –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∏ —Å–ª–µ–¥—É—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for sibling in [element.previous_sibling, element.next_sibling]:
            if sibling and hasattr(sibling, 'get_text'):
                text = sibling.get_text()
                date_match = re.search(r'\d{2}\.\d{2}\.\d{4}', text)
                if date_match:
                    return date_match.group()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —ç–ª–µ–º–µ–Ω—Ç
        parent = element.parent
        for _ in range(3):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 3 —É—Ä–æ–≤–Ω–µ–π –≤–≤–µ—Ä—Ö
            if parent:
                text = parent.get_text()
                date_match = re.search(r'\d{2}\.\d{2}\.\d{4}', text)
                if date_match:
                    return date_match.group()
                parent = parent.parent
        
        return "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

    def find_next_page(self, soup: BeautifulSoup, current_url: str) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        # –ò—â–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
        pagination = soup.find('ul', class_=['pagination', 'pager'])
        if pagination:
            next_links = pagination.find_all('a', text=lambda x: x and '>' in x)
            if not next_links:
                next_links = pagination.find_all('a', text=lambda x: x and '—Å–ª–µ–¥' in x.lower())
            
            for link in next_links:
                href = link.get('href')
                if href:
                    return urljoin(self.BASE_URL, href)
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
        next_btn = soup.find('a', class_=['next', 'page-next'])
        if next_btn and next_btn.get('href'):
            return urljoin(self.BASE_URL, next_btn.get('href'))
        
        return None

    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤"""
        if not text:
            return ""
        return ' '.join(text.split()).strip()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def get_parser():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ main.py"""
    return WebParser(limit=30)